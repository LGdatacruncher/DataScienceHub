{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**The objective of this project is to build a simple language model (LLM) using TensorFlow and Keras for sentiment analysis. The model will classify text into positive or negative sentiment by leveraging deep learning techniques such as embeddings, attention mechanisms, and dropout for robust performance.**\n"
      ],
      "metadata": {
        "id": "w32hgt8v_KUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z127DgTiZIH",
        "outputId": "6f280376-fbdd-4fd9-fced-7b789630ee2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf  # TensorFlow for building the neural network model\n",
        "from sklearn.model_selection import train_test_split  # For splitting the data into training and test sets\n",
        "import numpy as np  # For numerical operations\n",
        "\n",
        "# Example data (two sentences for each sentiment)\n",
        "texts = [\n",
        "    \"I love this product!\",  # Positive sentiment example\n",
        "    \"This is terrible.\",     # Negative sentiment example\n",
        "    \"Fantastic service!\",   # Positive sentiment example\n",
        "    \"I hate this experience.\"  # Negative sentiment example\n",
        "]\n",
        "\n",
        "# Corresponding labels: 1 for positive sentiment, 0 for negative sentiment\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.5, random_state=42)\n",
        "# `train_test_split` function divides the dataset into training and test sets\n",
        "# `test_size=0.5` means 50% of the data will be used for testing and 50% for training\n",
        "# `random_state=42` ensures that the split is reproducible\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=100, output_dim=8, input_length=5),  # Embedding layer for text\n",
        "    # `input_dim=100`: Vocabulary size (the maximum number of unique words considered)\n",
        "    # `output_dim=8`: Dimension of the dense embedding (size of the vector representation)\n",
        "    # `input_length=5`: Length of input sequences (pad or truncate to this length)\n",
        "\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),  # Pooling layer to reduce the dimensionality of the data\n",
        "    # `GlobalAveragePooling1D` calculates the average of the embeddings across the sequence length\n",
        "\n",
        "    tf.keras.layers.Dense(10, activation='relu'),  # Hidden layer with 10 neurons and ReLU activation function\n",
        "    # `Dense(10)`: Fully connected layer with 10 neurons\n",
        "    # `activation='relu'`: ReLU activation function to introduce non-linearity\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "    # `Dense(1)`: Single neuron output layer\n",
        "    # `activation='sigmoid'`: Sigmoid activation function for binary classification (outputs a probability between 0 and 1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# `optimizer='adam'`: Adam optimizer for adjusting weights\n",
        "# `loss='binary_crossentropy'`: Loss function for binary classification\n",
        "# `metrics=['accuracy']`: Accuracy metric to evaluate the model performance\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100)  # Limit vocab size to 100\n",
        "# `Tokenizer(num_words=100)`: Tokenizer instance with a maximum vocabulary size of 100\n",
        "tokenizer.fit_on_texts(X_train)  # Fit tokenizer on the training data\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)  # Convert text data to sequences of integers\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)  # Convert test text data to sequences of integers\n",
        "\n",
        "# Pad sequences\n",
        "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=5)  # Pad sequences to have the same length\n",
        "X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=5)  # Adjust `maxlen` to the desired length for sequences\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, np.array(y_train), epochs=5, validation_data=(X_test_pad, np.array(y_test)))\n",
        "# `X_train_pad`: Padded training sequences\n",
        "# `y_train`: Labels for training data\n",
        "# `epochs=5`: Number of epochs to train the model\n",
        "# `validation_data=(X_test_pad, np.array(y_test))`: Validation data to evaluate model performance after each epoch\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "results = model.evaluate(X_test_pad, np.array(y_test))\n",
        "# `evaluate` method calculates the loss and accuracy on the test data\n",
        "print(f\"Test Loss: {results[0]}\")  # Print the test loss\n",
        "print(f\"Test Accuracy: {results[1]}\")  # Print the test accuracy\n",
        "\n",
        "# Define a function to predict sentiment\n",
        "def predict_sentiment(text, tokenizer, model, max_len):\n",
        "    seq = tokenizer.texts_to_sequences([text])  # Convert text to sequence of integers\n",
        "    padded_seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)  # Pad the sequence\n",
        "    prediction = model.predict(padded_seq)  # Get the model's prediction\n",
        "    return 'Positive' if prediction[0] > 0.5 else 'Negative'  # Convert probability to sentiment label\n",
        "\n",
        "# Example usage\n",
        "text = \"I am happy with the service.\"\n",
        "print(f\"Sentiment: {predict_sentiment(text, tokenizer, model, 5)}\")  # Predict sentiment for a new sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mfoQO4FjB_6",
        "outputId": "7a753e2b-9253-43b4-95a4-95b17bc97445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 1s 981ms/step - loss: 0.6908 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6878 - accuracy: 1.0000 - val_loss: 0.7065 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6850 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6825 - accuracy: 1.0000 - val_loss: 0.7120 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6800 - accuracy: 1.0000 - val_loss: 0.7148 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7148 - accuracy: 0.0000e+00\n",
            "Test Loss: 0.7148082256317139\n",
            "Test Accuracy: 0.0\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "    \"I am not sure about this product.\",\n",
        "    \"Absolutely fantastic!\",\n",
        "    \"It’s an okay product.\",\n",
        "    \"I would not recommend this to anyone.\",\n",
        "    \"Very satisfied with my purchase.\"\n",
        "]\n",
        "labels = [0, 1, 0, 0, 1]  # 0: Negative, 1: Positive\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Tokenizer for text preprocessing\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 64\n",
        "num_heads = 2\n",
        "ff_dim = 128\n",
        "dropout_rate = 0.3\n",
        "max_len = 100\n",
        "\n",
        "# Define the model\n",
        "inputs = Input(shape=(max_len,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(embedding_layer, embedding_layer)\n",
        "x = LayerNormalization()(attention_output + embedding_layer)\n",
        "x = Dropout(dropout_rate)(x)  # Dropout layer to prevent overfitting\n",
        "ffn = Dense(ff_dim, activation='relu')(x)\n",
        "x = Dropout(dropout_rate)(ffn)  # Dropout layer to prevent overfitting\n",
        "x = LayerNormalization()(x + ffn)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Compile the model with optimizer, loss function, and evaluation metric\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "loss_fn = 'binary_crossentropy'\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(\n",
        "    padded_sequences, labels,\n",
        "    epochs=10,\n",
        "    batch_size=2,\n",
        "    validation_split=0.2,  # Added validation split\n",
        "    callbacks=[early_stopping]  # Added early stopping\n",
        ")\n",
        "\n",
        "# Test the model\n",
        "test_sentences = [\n",
        "    \"I am not sure about this product.\",\n",
        "    \"Absolutely fantastic!\",\n",
        "    \"It’s an okay product.\",\n",
        "    \"I would not recommend this to anyone.\",\n",
        "    \"Very satisfied with my purchase.\"\n",
        "]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n",
        "predictions = model.predict(test_padded_sequences)\n",
        "for sentence, prediction in zip(test_sentences, predictions):\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Sentence: '{sentence}' - Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgsIIZH8n_09",
        "outputId": "ca076996-1411-4920-8976-7bea36b80a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)     (None, 100, 64)              640000    ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 100, 64)              33216     ['embedding_6[0][0]',         \n",
            " ltiHeadAttention)                                                   'embedding_6[0][0]']         \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TF  (None, 100, 64)              0         ['multi_head_attention_4[0][0]\n",
            " OpLambda)                                                          ',                            \n",
            "                                                                     'embedding_6[0][0]']         \n",
            "                                                                                                  \n",
            " layer_normalization_8 (Lay  (None, 100, 64)              128       ['tf.__operators__.add_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)         (None, 100, 64)              0         ['layer_normalization_8[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_15 (Dense)            (None, 100, 128)             8320      ['dropout_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)        (None, 100, 128)             0         ['dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TF  (None, 100, 128)             0         ['dropout_10[0][0]',          \n",
            " OpLambda)                                                           'dense_15[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_9 (Lay  (None, 100, 128)             256       ['tf.__operators__.add_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4  (None, 128)                  0         ['layer_normalization_9[0][0]'\n",
            "  (GlobalAveragePooling1D)                                          ]                             \n",
            "                                                                                                  \n",
            " dense_16 (Dense)            (None, 1)                    129       ['global_average_pooling1d_4[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 682049 (2.60 MB)\n",
            "Trainable params: 682049 (2.60 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 2s 366ms/step - loss: 0.7349 - accuracy: 0.2500 - val_loss: 0.7254 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.6134 - accuracy: 0.7500 - val_loss: 0.9482 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.5870 - accuracy: 0.7500 - val_loss: 1.2334 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 207ms/step\n",
            "Sentence: 'I am not sure about this product.' - Sentiment: Negative\n",
            "Sentence: 'Absolutely fantastic!' - Sentiment: Negative\n",
            "Sentence: 'It’s an okay product.' - Sentiment: Positive\n",
            "Sentence: 'I would not recommend this to anyone.' - Sentiment: Negative\n",
            "Sentence: 'Very satisfied with my purchase.' - Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4C3jLxfoV4I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}